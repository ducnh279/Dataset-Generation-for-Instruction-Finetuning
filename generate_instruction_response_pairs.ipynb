{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxMxDwQAGO5w"
   },
   "source": [
    "# Generate Instruction-Response Pairs\n",
    "\n",
    "- This notebook uses Phi-3 3.8B parameters instruct model to generate a synthetic dataset.\n",
    "\n",
    "- The dataset creation follows the \"hack\" proposed in the paper \"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing\": https://arxiv.org/abs/2406.08464\n",
    "\n",
    "- The dataset creation includes 2 steps:\n",
    "    - Step 1: Instruction generation\n",
    "    - Step 2: Response generation\n",
    "\n",
    "- The generated dataset will be an instruction dataset with \"instruction\" and \"output\" fields, similar to what can be found in Alpaca:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"What is the capital of Vietnam?\",\n",
    "    \"output\": \"The capital of Vietnam is Hanoi.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUQMwXjcL9IX"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Ze_1UL_68hS"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYk4z8K5MALK"
   },
   "source": [
    "# Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IfXVnlKaMKeC"
   },
   "outputs": [],
   "source": [
    "model_id = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198,
     "referenced_widgets": [
      "a937825f6d4244ebabfb643dc9e56c2f",
      "d9f9fc99cad8432a96e36a970af935aa",
      "47788b6150d244048dd4143cf82d4b1b",
      "82717aa38b4e4fb5933256814a710aba",
      "5940373a967743fab8c5bc76ff676131",
      "ac736c9fd16d4e179f1707e5b5a72ee0",
      "83b16c4acb554b4b9a949cfb8b77c40a",
      "9cf69003907e40fdbecf5bc7ea29b1d6",
      "bac27c5c8d3a49da992d38b2fe118074",
      "e8476169df96415ab872ad8a0e673cb8",
      "56609bc83c194fe2a520c050a384385f"
     ]
    },
    "id": "RO-713DY60GS",
    "outputId": "5c5e7649-f86d-4cd9-db6e-1e083cd665b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a937825f6d4244ebabfb643dc9e56c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=device,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To1EtgFbMZPu"
   },
   "source": [
    "# Examine phi-3 chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lvZvtNxk7gJG"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': 'What is the capital of Vietnam?'},\n",
    "    {'role': 'assistant', 'content': 'The capital of Vietnam is Hanoi.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LGjhyfwj9bOt",
    "outputId": "ff370c44-19b6-4f26-aed1-f8012dad020c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<s><|user|> What is the capital of Vietnam?<|end|><|assistant|> The capital of Vietnam is Hanoi.<|end|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.apply_chat_template(messages)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-BcQhvOMmxW"
   },
   "source": [
    "# Step 1: Instruction Generation\n",
    "\n",
    "With the chat template below, to generate an instruction, we only need to pass the input as `<s>|user|` and set a stop criterion as `|end|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNlGooX4QcoR",
    "outputId": "8b742a8e-280c-4efa-de7b-a0b7c17f0c1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32007]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<|end|>', add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1orNZJv8Qlbf"
   },
   "outputs": [],
   "source": [
    "stop_on_tokens = [32007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LUo_37QRBMiM"
   },
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [32007]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ulQAf0CrANY7"
   },
   "outputs": [],
   "source": [
    "def generate_instruction(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        pre_query_template,\n",
    "        stopping_criteria,\n",
    "        device\n",
    "    ):\n",
    "    encodings = tokenizer(pre_query_template, return_tensors='pt', add_special_tokens=False).to(device)\n",
    "\n",
    "    tokens = model.generate(\n",
    "        input_ids=encodings['input_ids'],\n",
    "        max_new_tokens=1024,\n",
    "        temperature=1.,\n",
    "        top_p=1.,\n",
    "        do_sample=True,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0]\n",
    "\n",
    "    instruction = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UobMPh76BCO9",
    "outputId": "ee27f120-5367-49ac-b5d0-767bc96ea845"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction: I need a C header for a software package that's like a suite of utilities. Could you set up constants for version info as four strings, and define numeric constants for status, which should be okay only between 0 and 1, and error codes with descriptions that reset on minor updates. There should be version formatting for major and minor to a string, a string for default config, and functions to get this string and a description. Also, include a default numeric timeout, a function to print errors with code descriptions, a logging function with different verbosity levels, and preprocessor macros for conditional compilation. Lastly, ensure the protection of everything against multiple inclusions and add comments for documentation. The version and error descriptions should be easily changeable.\n"
     ]
    }
   ],
   "source": [
    "instruction = generate_instruction(model=model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     pre_query_template='<s><|user|>',\n",
    "                     stopping_criteria=stopping_criteria,\n",
    "                     device=device)\n",
    "\n",
    "print(f'Generated instruction: {instruction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn755U7MTMzC"
   },
   "source": [
    "# Step 2: Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jBSm3X6nBHif"
   },
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, instruction, device):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': instruction}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    tokens = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=1.,\n",
    "        top_p=1.,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0]\n",
    "    instruction_and_response = tokenizer.decode(tokens)\n",
    "    response = instruction_and_response.split('<|assistant|>')[-1][:-len('<|end|>')]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVx15TNeTqky",
    "outputId": "527557fe-000d-4661-ec96-4405a8e443ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:  Certainly! Here's how you can incorporate the changes:\n",
      "\n",
      "```c\n",
      "// Start of the header file\n",
      "#ifndef SOFTWARE_UTILITIES_H\n",
      "#define SOFTWARE_UTILITIES_H\n",
      "// ... [Rest of the predefined macros and includes]\n",
      "\n",
      "// Define version in a separate header if needed\n",
      "// #include \"software_version.h\"\n",
      "\n",
      "// Error codes array\n",
      "#define ERROR_CODES [(int*)(^)([0 \"InvalidParameter\"]) ..., NULL]\n",
      "\n",
      "// End of the version-related definitions\n",
      "\n",
      "// Function to check if a numeric code refers to a valid error\n",
      "int isValidErrorCode(int code);\n",
      "\n",
      "// ... [Other function declarations]\n",
      "\n",
      "// End of the utility functions\n",
      "\n",
      "// Define a macro for preprocessing a config path\n",
      "#define CONFIG_PATH_PREPROCESSOR(path) #path // Simple example for string concatenation\n",
      "\n",
      "// End of the header file\n",
      "#endif // SOFTWARE_UTILITIES_H\n",
      "\n",
      "// In your source file, implement the isValidErrorCode function\n",
      "int isValidErrorCode(int code) {\n",
      "    return (code > DEFAULT_NAKED_ERROR && code <= MAX_NAKED_ERROR);\n",
      "}\n",
      "```\n",
      "I've shown you how to declare an array of strings where you can store your error codes and added the function prototype for `isValidErrorCode`. To check an error code's validity, simply use the `isValidErrorCode` function.\n",
      "\n",
      "For dynamic logging levels and config preprocessing, you would need to add logic in your implementation files. You could use macros and configuration files from which you define logging levels and paths that can be parsed at compile-time. Make sure to define and implement appropriate error handling for the dynamic part according to your needs.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(model, tokenizer, instruction, device)\n",
    "\n",
    "print(f'Generated response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoZGPdVaTvYI",
    "outputId": "3728ff85-e87e-4340-83f2-ede761735c8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': \"I need a C header for a software package that's like a suite of utilities. Could you set up constants for version info as four strings, and define numeric constants for status, which should be okay only between 0 and 1, and error codes with descriptions that reset on minor updates. There should be version formatting for major and minor to a string, a string for default config, and functions to get this string and a description. Also, include a default numeric timeout, a function to print errors with code descriptions, a logging function with different verbosity levels, and preprocessor macros for conditional compilation. Lastly, ensure the protection of everything against multiple inclusions and add comments for documentation. The version and error descriptions should be easily changeable.\",\n",
       "  'response': ' Certainly! Here\\'s how you can incorporate the changes:\\n\\n```c\\n// Start of the header file\\n#ifndef SOFTWARE_UTILITIES_H\\n#define SOFTWARE_UTILITIES_H\\n// ... [Rest of the predefined macros and includes]\\n\\n// Define version in a separate header if needed\\n// #include \"software_version.h\"\\n\\n// Error codes array\\n#define ERROR_CODES [(int*)(^)([0 \"InvalidParameter\"]) ..., NULL]\\n\\n// End of the version-related definitions\\n\\n// Function to check if a numeric code refers to a valid error\\nint isValidErrorCode(int code);\\n\\n// ... [Other function declarations]\\n\\n// End of the utility functions\\n\\n// Define a macro for preprocessing a config path\\n#define CONFIG_PATH_PREPROCESSOR(path) #path // Simple example for string concatenation\\n\\n// End of the header file\\n#endif // SOFTWARE_UTILITIES_H\\n\\n// In your source file, implement the isValidErrorCode function\\nint isValidErrorCode(int code) {\\n    return (code > DEFAULT_NAKED_ERROR && code <= MAX_NAKED_ERROR);\\n}\\n```\\nI\\'ve shown you how to declare an array of strings where you can store your error codes and added the function prototype for `isValidErrorCode`. To check an error code\\'s validity, simply use the `isValidErrorCode` function.\\n\\nFor dynamic logging levels and config preprocessing, you would need to add logic in your implementation files. You could use macros and configuration files from which you define logging levels and paths that can be parsed at compile-time. Make sure to define and implement appropriate error handling for the dynamic part according to your needs.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\n",
    "    {'instruction': instruction, 'response': response}\n",
    "]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PHF4FW4VLYZ"
   },
   "source": [
    "# Combine 2 steps to create an instruction dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NjF5Hg-U-EF",
    "outputId": "75bebb47-8602-444e-eeb6-73f6bdf27ab9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:54<00:00, 38.12s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "dataset_size = 3\n",
    "\n",
    "for _ in trange(dataset_size):\n",
    "    instruction = generate_instruction(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        pre_query_template='<s><|user|>',\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    response = generate_response(model, tokenizer, instruction, device)\n",
    "\n",
    "    entry = {\n",
    "        'instruction': instruction,\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "    dataset.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UWYy6EcDWJ3D"
   },
   "outputs": [],
   "source": [
    "with open(\"instruction-data-phi3.json\", mode=\"w\") as file:\n",
    "    json.dump(dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91SieYg9YI05",
    "outputId": "94c44216-7d82-4326-fed4-90781cbe549f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': \"I need your help setting up a Scala project using functional programming libraries. I want to convert HTML structures into reactive components with some specific functionalities. Could you provide code to create a package for reactive HTML parsing that includes:\\n\\n1. A utility to parse optional HTML content that can fail, returning some results.\\n2. A function for parsing an HTML string input to extract and generate reactive components based on the content.\\n3. A structure that represents parsed HTML, containing raw string data and reactive components with HTML content and a boolean flag.\\n4. Methods to extract the raw HTML from this structure, both with and without trimming whitespace.\\n\\nIt's crucial to include proper error handling and comments. Can you supply me just with the Scala code for these requirements?\",\n",
       "  'response': ' ```scala\\nclass HtmlParsingUtils(implicit val sa: Scalatags) extends HtmlParser {\\n  def parseOrFail(htmlOrEmptyString: Option[String]): HtmlParserResult = {\\n    // unchanged code...\\n  }\\n\\n  // Adding caching to HtmlNode\\n  private val cache = new mutable.WeakHashMap[String, HtmlNode]()\\n  \\n  def parseHtml(html: String): HtmlParserResult = {\\n    Option(cache.get(html)).getOrElse {\\n      val parsed = ... // existing parsing logic\\n      cache.put(html, parsed)\\n      parsed\\n    }\\n  }\\n\\n  // unchanged code...\\n  \\n  def extractRawHtml: Option[String] = {\\n    new HtmlParser().extractRawHtml\\n  }\\n}\\n\\nobject HtmlParser {\\n  def extractFromResult(result: HtmlParserResult): Option[String] = ...\\n}\\n```'},\n",
       " {'instruction': 'I\\'m working on a CMake script for an embedded application which has specific directories relative to the project root. The project uses a Cortex-M4 processor, with a focus on floating-point hardware support. I need to define a list of source files, covering various functionalities such as core components, utility functions, UART communication, and interrupt handling. There should be a variable for source files specifically for UART communication, and source files located within a \\'sources\\' directory under the project root. The script should also be able to conditionally include additional source files if some features like flash file system support or DMA-to-SRAM are enabled. Plus, it should incorporate the configuration generated by CMake for platform-specific parts. I have this snippet to get started:\\n\\n```cmake\\nset(SOURCE_FILES\\n\\t\"source_file_1.cpp\"\\n\\t# ... More source files here ...\\n)\\n```',\n",
       "  'response': ' Sure! To improve maintainability, you can use variables for the different groups of source files like this:\\n\\n```cmake\\n# Paths to source directories\\nset(CORE_SOURCES \"path/to/source_helpers.cpp\"\\n                 \"path/to/source_main.cpp\")\\n# ... other source group definitions ...\\n\\n# Organizing source files\\nset(SOURCE_FILES\\n\\t${CORE_SOURCES}\\n\\t# ... other grouped source files ...\\n\\t\"src/uart/source_uart.cpp\"\\n)\\n```\\n\\nFor the conditional compiler flags and preprocessor definition:\\n\\n```cmake\\nif(ENABLE_FLASH)\\n    add_definitions(-DFLASH)\\n    # Define compiler flags for flash features\\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DFLASH_FEATURE_X\")\\nendif()\\n```\\n\\nRemember to add these changes in context with the rest of the CMake script you already have to integrate smoothly.'},\n",
       " {'instruction': 'python program use random to shuffle a list of integers and find smallest unique number after shuffling',\n",
       "  'response': ' Certainly! To solve this problem, we\\'ll follow these steps:\\n\\n1. Shuffle a list of integers using Python\\'s `random.shuffle` method.\\n2. Find the smallest unique number in the shuffled list. If there is no unique number, we handle that scenario as well.\\n\\nHere\\'s a Python program that accomplishes this:\\n\\n```python\\nimport random\\n\\ndef find_smallest_unique(numbers):\\n    \"\"\"\\n    Shuffle the list and find the smallest unique number.\\n    Returns the smallest unique number, or a message if no unique numbers exist.\\n    \"\"\"\\n    # Shuffle the list\\n    random.shuffle(numbers)\\n    \\n    # Step 2: Find the smallest unique number\\n    smallest_unique = None\\n    for num in numbers:\\n        if numbers.count(num) == 1:  # Check if the number appears once\\n            if smallest_unique is None or num < smallest_unique:\\n                smallest_unique = num\\n    \\n    # Check if we found a unique number\\n    if smallest_unique is not None:\\n        return f\"The smallest unique number is: {smallest_unique}\"\\n    else:\\n        return \"No unique number found after shuffling.\"\\n\\n# Example usage\\nnumbers = [5, 3, 6, 5, 2, 3, 1]\\nprint(find_smallest_unique(numbers))\\n```\\n\\nHowever, the above implementation can be inefficient for large lists because `numbers.count(num)` runs in O(n) time where n is the size of the list, leading to an overall O(n^2) time complexity for finding the smallest unique number.\\n\\nTo optimize, we can maintain a list of counts for each number as we go (`collections.Counter`), then iterate through the original numbers to find the smallest unique one based on the counts list, achieving an overall better time complexity for larger lists:\\n\\n```python\\nfrom collections import Counter\\nimport random\\n\\ndef find_smallest_unique_optimized(numbers):\\n    \"\"\"\\n    Shuffle the list, count each number, and find the smallest unique number.\\n    Returns the smallest unique number, or a message if no unique numbers exist.\\n    \"\"\"\\n    # Count occurrences of each number\\n    counts = Counter(numbers)\\n    \\n    # Find the smallest number that only appears once\\n    smallest_unique = None\\n    for num in numbers:\\n        if counts[num] == 1:\\n            if smallest_unique is None or num < smallest_unique:\\n                smallest_unique = num\\n    \\n    # Return the result\\n    if smallest_unique is not None:\\n        return f\"The smallest unique number is: {smallest_unique}\"\\n    else:\\n        return \"No unique number found after shuffling.\"\\n\\n# Example usage\\nnumbers = [5, 3, 6, 5, 2, 3, 1]\\nprint(find_smallest_unique_optimized(numbers))\\n```\\n\\nThis optimized code uses `collections.Counter` to efficiently determine if each number is unique and maintains a `smallest_unique` candidate throughout, thereby improving performance on larger datasets.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IUZsKgXiZe13"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "scratchpad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "47788b6150d244048dd4143cf82d4b1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cf69003907e40fdbecf5bc7ea29b1d6",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bac27c5c8d3a49da992d38b2fe118074",
      "value": 2
     }
    },
    "56609bc83c194fe2a520c050a384385f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5940373a967743fab8c5bc76ff676131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82717aa38b4e4fb5933256814a710aba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8476169df96415ab872ad8a0e673cb8",
      "placeholder": "​",
      "style": "IPY_MODEL_56609bc83c194fe2a520c050a384385f",
      "value": " 2/2 [00:42&lt;00:00, 19.78s/it]"
     }
    },
    "83b16c4acb554b4b9a949cfb8b77c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cf69003907e40fdbecf5bc7ea29b1d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a937825f6d4244ebabfb643dc9e56c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9f9fc99cad8432a96e36a970af935aa",
       "IPY_MODEL_47788b6150d244048dd4143cf82d4b1b",
       "IPY_MODEL_82717aa38b4e4fb5933256814a710aba"
      ],
      "layout": "IPY_MODEL_5940373a967743fab8c5bc76ff676131"
     }
    },
    "ac736c9fd16d4e179f1707e5b5a72ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bac27c5c8d3a49da992d38b2fe118074": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9f9fc99cad8432a96e36a970af935aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac736c9fd16d4e179f1707e5b5a72ee0",
      "placeholder": "​",
      "style": "IPY_MODEL_83b16c4acb554b4b9a949cfb8b77c40a",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e8476169df96415ab872ad8a0e673cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
